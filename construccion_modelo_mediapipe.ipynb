{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d157e54",
   "metadata": {},
   "source": [
    "# Construcci√≥n del modelo en mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c3545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øCUDA disponible? True\n",
      "GPU actual: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n que se este utilizando la GPU\n",
    "import torch\n",
    "print(\"¬øCUDA disponible?\", torch.cuda.is_available())\n",
    "print(\"GPU actual:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148d43c",
   "metadata": {},
   "source": [
    "\n",
    "### üîπ Explicaci√≥n de Importaciones\n",
    "\n",
    "#### **Librer√≠as est√°ndar y manejo de archivos**\n",
    "\n",
    "* **`os`**\n",
    "\n",
    "  * Utilizada para interactuar con el sistema operativo (manejar rutas, listar directorios y archivos).\n",
    "  * Ejemplo de uso: `os.listdir()`, `os.path.join()`.\n",
    "\n",
    "#### **Procesamiento de Im√°genes y Videos**\n",
    "\n",
    "* **`cv2`** (`opencv-python`)\n",
    "\n",
    "  * Se utiliza para la lectura, escritura y redimensionamiento de im√°genes extra√≠das de los videos.\n",
    "  * Ejemplo de uso: `cv2.imread()`, `cv2.resize()`.\n",
    "\n",
    "#### **C√≥mputo num√©rico y manejo de datos**\n",
    "\n",
    "* **`numpy as np`**\n",
    "\n",
    "  * Se emplea para manejar arrays multidimensionales, especialmente para cargar vectores guardados en formato `.npy`.\n",
    "  * Ejemplo de uso: `np.load()`, `np.array()`.\n",
    "\n",
    "#### **Herramientas de Machine Learning y Deep Learning (PyTorch)**\n",
    "\n",
    "* **`torch`**\n",
    "\n",
    "  * Framework central para la creaci√≥n y entrenamiento del modelo.\n",
    "  * Ejemplo de uso: Tensores (`torch.tensor()`), c√°lculo de gradientes (`loss.backward()`).\n",
    "\n",
    "* **`torch.utils.data`** (`Dataset`, `DataLoader`)\n",
    "\n",
    "  * `Dataset`: Clase base para definir un conjunto personalizado de datos (manejo de muestras y etiquetas).\n",
    "  * `DataLoader`: Herramienta para cargar datos en batches durante el entrenamiento.\n",
    "\n",
    "* **`torchvision.transforms`**\n",
    "\n",
    "  * Utilizado para realizar transformaciones comunes en im√°genes (por ejemplo, conversi√≥n de im√°genes a tensores).\n",
    "\n",
    "* **`torchvision.models as models`**\n",
    "\n",
    "  * Proporciona modelos preentrenados, en este caso EfficientNet B0, que se utiliza como extractor de caracter√≠sticas visuales.\n",
    "\n",
    "* **`torch.nn as nn`**\n",
    "\n",
    "  * Define bloques fundamentales de redes neuronales (capas convolucionales, lineales, funciones de activaci√≥n, etc.).\n",
    "  * Ejemplo de uso: `nn.LSTM`, `nn.Linear`, `nn.ReLU`, `nn.Sigmoid`.\n",
    "\n",
    "* **`torch.optim` (Adam)**\n",
    "\n",
    "  * Adam es un optimizador eficiente usado para actualizar los par√°metros del modelo durante el entrenamiento.\n",
    "\n",
    "#### **Evaluaci√≥n del Modelo**\n",
    "\n",
    "* **`sklearn.metrics` (`accuracy_score`, `f1_score`, `classification_report`)**\n",
    "\n",
    "  * M√©tricas utilizadas para evaluar el rendimiento del modelo.\n",
    "  \n",
    "#### **Utilidades diversas**\n",
    "\n",
    "* **`sklearn.utils.shuffle`**\n",
    "\n",
    "  * Se utiliza para mezclar aleatoriamente los datos antes de dividirlos en entrenamiento y validaci√≥n.\n",
    "\n",
    "* **`collections.Counter`**\n",
    "\n",
    "  * Herramienta para contar ocurrencias de elementos en listas, usada aqu√≠ para mostrar distribuci√≥n de clases y predicciones del modelo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1feff47",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### **Instalaci√≥n de Dependencias**\n",
    "\n",
    "A continuaci√≥n, se presentan los comandos necesarios para instalar todas las librer√≠as usadas en este notebook. Se recomienda hacerlo dentro de un entorno virtual:\n",
    "\n",
    "```bash\n",
    "# Librer√≠as b√°sicas para manipulaci√≥n de im√°genes y arrays\n",
    "pip install opencv-python\n",
    "pip install numpy\n",
    "\n",
    "# PyTorch (GPU o CPU seg√∫n disponibilidad)\n",
    "# Si tienes CUDA disponible:\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "\n",
    "# Si solo tienes CPU (sin CUDA):\n",
    "pip install torch torchvision torchaudio\n",
    "\n",
    "# Librer√≠as de Machine Learning adicionales\n",
    "pip install scikit-learn\n",
    "\n",
    "# Utilidad adicional (opcional, generalmente instalada por defecto)\n",
    "pip install collections\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è **Configuraci√≥n recomendada de entorno virtual**\n",
    "\n",
    "Es recomendable crear un entorno virtual para manejar dependencias sin conflictos:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "\n",
    "# Activar entorno virtual:\n",
    "# Windows\n",
    ".venv\\Scripts\\activate\n",
    "\n",
    "# Linux / MacOS\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "Luego ejecuta los comandos de instalaci√≥n arriba mencionados.\n",
    "\n",
    "---\n",
    "\n",
    "**Nota importante:**\n",
    "\n",
    "* **PyTorch** tiene dos variantes principales seg√∫n si dispones de una GPU con CUDA o solo CPU. Aseg√∫rate de seleccionar la instalaci√≥n correspondiente a tu hardware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40c50bd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Explicaci√≥n del Dataset Personalizado**\n",
    "\n",
    "#### üîπ Clase `DeepfakeDataset`\n",
    "\n",
    "Esta clase personalizada hereda de la clase base `torch.utils.data.Dataset` y es utilizada para cargar y gestionar eficientemente el conjunto de datos del proyecto, que consiste en im√°genes de rostros y vectores extra√≠dos con MediaPipe. Su principal objetivo es facilitar el manejo estructurado de secuencias de im√°genes (frames) y vectores num√©ricos para entrenamiento y validaci√≥n del modelo de detecci√≥n de deepfakes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **M√©todos Principales**\n",
    "\n",
    "##### **1. Constructor (`__init__`)**\n",
    "\n",
    "**Par√°metros:**\n",
    "\n",
    "* **`base_path`**: Ruta ra√≠z que contiene las carpetas de datos (\"original\" y diversas carpetas de deepfakes).\n",
    "* **`sequence_length=16`**: N√∫mero de im√°genes (frames) consecutivas utilizadas para representar cada video.\n",
    "* **`split='train'`**: Determina si el conjunto se utiliza para entrenamiento (`train`) o validaci√≥n (`val`).\n",
    "* **`val_split=0.2`**: Proporci√≥n de datos reservados para la validaci√≥n.\n",
    "\n",
    "**Funcionamiento interno:**\n",
    "\n",
    "* Carga todos los videos completos utilizando la funci√≥n interna `_collect_video_ids()`.\n",
    "* Mezcla aleatoriamente todos los videos para evitar sesgos en la divisi√≥n.\n",
    "* Realiza la divisi√≥n del dataset a nivel video, no por imagen, garantizando que im√°genes del mismo video nunca est√©n simult√°neamente en entrenamiento y validaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "##### **2. M√©todo privado `_collect_video_ids(base_path)`**\n",
    "\n",
    "Esta funci√≥n realiza un recorrido estructurado sobre los datos:\n",
    "\n",
    "**Funcionamiento:**\n",
    "\n",
    "* Itera sobre cada carpeta en `base_path`.\n",
    "* Determina la etiqueta (`label`):\n",
    "\n",
    "  * `0`: si la carpeta es `\"original\"` (videos reales).\n",
    "  * `1`: si es cualquier otra carpeta (videos falsificados o deepfake).\n",
    "* Agrupa im√°genes por ID de video (por ejemplo, `video123_0.jpg`, `video123_1.jpg`, etc.).\n",
    "* Selecciona √∫nicamente los videos que contienen al menos tantas im√°genes como la `sequence_length` requerida, descartando videos incompletos.\n",
    "\n",
    "**Devuelve:**\n",
    "\n",
    "* Una lista de tuplas con la estructura `(ruta_carpeta, video_id, etiqueta)`.\n",
    "\n",
    "---\n",
    "\n",
    "##### **3. M√©todo `__len__(self)`**\n",
    "\n",
    "* Simplemente devuelve la cantidad total de videos disponibles en el conjunto actual (`train` o `val`).\n",
    "\n",
    "---\n",
    "\n",
    "##### **4. M√©todo `__getitem__(self, idx)`**\n",
    "\n",
    "Este es el m√©todo clave, llamado por `DataLoader` para obtener una muestra individual del conjunto de datos:\n",
    "\n",
    "**Funcionamiento:**\n",
    "\n",
    "* Recupera el video correspondiente al √≠ndice dado (`idx`).\n",
    "* Para cada video, carga exactamente `sequence_length` im√°genes (`.jpg`) y sus correspondientes vectores de landmarks faciales (`.npy`).\n",
    "* Cada imagen se carga usando OpenCV (`cv2.imread()`), redimensionada a `256x256`, y transformada en un tensor mediante `torchvision.transforms`.\n",
    "* Los landmarks (vectores num√©ricos) se cargan directamente desde archivos `.npy`.\n",
    "* Finalmente, apila todas las im√°genes y landmarks en tensores con dimensiones:\n",
    "\n",
    "  * Im√°genes: `(sequence_length, 3, 256, 256)`\n",
    "  * Landmarks: `(sequence_length, 5)`\n",
    "* Devuelve una tupla: `(tensor_im√°genes, tensor_landmarks, etiqueta)`\n",
    "\n",
    "---\n",
    "\n",
    "### **Funci√≥n de diagn√≥stico `print_class_distribution(dataset, name)`**\n",
    "\n",
    "Funci√≥n utilitaria adicional, utilizada para verificar la distribuci√≥n de clases (real vs fake) en un conjunto de datos dado:\n",
    "\n",
    "**Funcionamiento:**\n",
    "\n",
    "* Recorre el dataset proporcionado, acumulando las etiquetas.\n",
    "* Utiliza la clase `Counter` para contar cu√°ntas muestras hay de cada clase (`0`: real, `1`: fake).\n",
    "* Muestra claramente en pantalla la distribuci√≥n de clases, √∫til para verificar el balance del dataset antes del entrenamiento.\n",
    "\n",
    "**Ejemplo de salida:**\n",
    "\n",
    "```\n",
    "Train class distribution:\n",
    "  Real (0): 1200\n",
    "  Fake (1): 800\n",
    "------------------------------\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Resumen de uso:**\n",
    "\n",
    "Al utilizar la clase `DeepfakeDataset`, se asegura:\n",
    "\n",
    "* **Consistencia en las muestras:** Cada muestra devuelta contiene siempre secuencias completas, facilitando entrenamiento y evaluaci√≥n del modelo.\n",
    "* **Separaci√≥n correcta:** Los videos de entrenamiento y validaci√≥n nunca se mezclan, evitando la fuga de datos (data leakage).\n",
    "* **Control de calidad:** La funci√≥n de diagn√≥stico permite r√°pidamente verificar que las clases est√©n equilibradas y listas para el entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, base_path, sequence_length=16, split='train', val_split=0.2):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.base_path = base_path\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        # Paso 1: Cargar videos completos con etiquetas\n",
    "        all_videos = self._collect_video_ids(base_path)\n",
    "        all_videos = shuffle(all_videos, random_state=42)  # Mezcla antes de dividir\n",
    "        \n",
    "        # Paso 2: Dividir por video, no por muestra\n",
    "        split_idx = int(len(all_videos) * (1 - val_split))\n",
    "        self.videos = all_videos[:split_idx] if split == 'train' else all_videos[split_idx:]\n",
    "\n",
    "    def _collect_video_ids(self, base_path):\n",
    "        video_samples = []\n",
    "        for folder in os.listdir(base_path):\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            label = 0 if folder == \"original\" else 1\n",
    "            video_ids = {}\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".jpg\"):\n",
    "                    vid = \"_\".join(file.split(\"_\")[:-1])\n",
    "                    video_ids.setdefault(vid, []).append(file)\n",
    "            for vid, files in video_ids.items():\n",
    "                if len(files) >= self.sequence_length:\n",
    "                    video_samples.append((folder_path, vid, label))\n",
    "        return video_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder, video_id, label = self.videos[idx]\n",
    "        imgs = []\n",
    "        lmks = []\n",
    "        for i in range(self.sequence_length):\n",
    "            img_path = os.path.join(folder, f\"{video_id}_{i}.jpg\")\n",
    "            lmk_path = os.path.join(folder, f\"{video_id}_{i}.npy\")\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (256, 256))\n",
    "            img = self.transform(img)  # (3, 256, 256)\n",
    "            lmk = torch.tensor(np.load(lmk_path), dtype=torch.float32)\n",
    "            imgs.append(img)\n",
    "            lmks.append(lmk)\n",
    "        imgs = torch.stack(imgs)      # (16, 3, 256, 256)\n",
    "        lmks = torch.stack(lmks)      # (16, 5)\n",
    "        return imgs, lmks, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "# ‚úÖ Diagn√≥stico: Conteo de clases reales/falsas\n",
    "def print_class_distribution(dataset, name):\n",
    "    labels = [int(label.item()) for _, _, label in dataset]\n",
    "    counts = Counter(labels)\n",
    "    print(f\"{name} class distribution:\")\n",
    "    print(f\"  Real (0): {counts.get(0, 0)}\")\n",
    "    print(f\"  Fake (1): {counts.get(1, 0)}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907009d",
   "metadata": {},
   "source": [
    "\n",
    "### **Explicaci√≥n del Modelo DeepfakeDetector**\n",
    "\n",
    "La clase `DeepfakeDetector` implementa una red neuronal h√≠brida que combina extracci√≥n de caracter√≠sticas mediante redes convolucionales preentrenadas y modelado secuencial mediante redes recurrentes (LSTM). El modelo est√° dise√±ado espec√≠ficamente para detectar videos falsificados (deepfakes) analizando secuencias de im√°genes junto con vectores num√©ricos asociados a landmarks faciales.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Arquitectura del Modelo**\n",
    "\n",
    "La arquitectura est√° compuesta principalmente por tres partes:\n",
    "\n",
    "1. **CNN (EfficientNet-B0)**\n",
    "2. **LSTM bidireccional**\n",
    "3. **Capa de clasificaci√≥n (Fully Connected)**\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Red Convolucional CNN (EfficientNet-B0)**\n",
    "\n",
    "```python\n",
    "self.cnn = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "self.cnn.classifier = nn.Identity()  # elimina la capa final\n",
    "```\n",
    "\n",
    "**Descripci√≥n:**\n",
    "\n",
    "* Utiliza el modelo EfficientNet-B0 preentrenado en ImageNet, que es altamente eficiente en t√©rminos de precisi√≥n y tama√±o.\n",
    "* Se remueve la capa clasificadora original (√∫ltima capa fully-connected) mediante `nn.Identity()`. Esto permite usar EfficientNet como extractor de caracter√≠sticas visuales (embedding visual de dimensi√≥n 1280).\n",
    "\n",
    "**Resultado:**\n",
    "Cada imagen pasa por EfficientNet-B0, generando un vector de caracter√≠sticas de tama√±o **1280**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Red Recurrente LSTM Bidireccional**\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(\n",
    "    input_size=1285,  # embedding visual (1280) + landmarks (5)\n",
    "    hidden_size=128,\n",
    "    num_layers=1,\n",
    "    batch_first=True,\n",
    "    bidirectional=True\n",
    ")\n",
    "```\n",
    "\n",
    "**Descripci√≥n:**\n",
    "\n",
    "* La red LSTM es capaz de capturar dependencias temporales entre frames consecutivos del video.\n",
    "* El `input_size` de 1285 corresponde a la combinaci√≥n de:\n",
    "\n",
    "  * **1280 dimensiones** del embedding visual generado por EfficientNet.\n",
    "  * **5 dimensiones** del vector de landmarks faciales obtenidos con MediaPipe.\n",
    "* Es bidireccional, permitiendo modelar informaci√≥n temporal en ambas direcciones (adelante y atr√°s en la secuencia).\n",
    "* El resultado es un vector combinado de tama√±o `256` (128 dimensiones √ó 2 direcciones).\n",
    "\n",
    "**Resultado:**\n",
    "Captura informaci√≥n din√°mica y temporal para distinguir patrones entre videos reales y falsificados.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Clasificaci√≥n mediante capa Fully-Connected**\n",
    "\n",
    "```python\n",
    "self.fc = nn.Sequential(\n",
    "    nn.Linear(256, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "```\n",
    "\n",
    "**Descripci√≥n:**\n",
    "\n",
    "* Esta capa recibe el vector resultante del LSTM y produce una predicci√≥n binaria (real o falso).\n",
    "* Contiene:\n",
    "\n",
    "  * Una capa lineal que reduce dimensionalidad de 256 ‚Üí 64.\n",
    "  * Activaci√≥n ReLU para introducir no-linealidad.\n",
    "  * Dropout de `0.3` para reducir sobreajuste (overfitting).\n",
    "  * Una capa lineal final que reduce la dimensionalidad de 64 ‚Üí 1.\n",
    "  * Activaci√≥n Sigmoid para convertir la salida en una probabilidad entre 0 y 1 (clasificaci√≥n binaria).\n",
    "\n",
    "**Resultado:**\n",
    "Una probabilidad indicando si el video es real (`0`) o falsificado (`1`).\n",
    "\n",
    "---\n",
    "\n",
    "### **M√©todo `forward(x_imgs, x_lmks)`**\n",
    "\n",
    "El m√©todo `forward` define c√≥mo fluye la informaci√≥n a trav√©s del modelo durante la inferencia o entrenamiento:\n",
    "\n",
    "**Par√°metros:**\n",
    "\n",
    "* **`x_imgs`**: Tensor con dimensiones `(Batch, Tiempo, Canales, Alto, Ancho)` que contiene las secuencias de im√°genes.\n",
    "* **`x_lmks`**: Tensor con dimensiones `(Batch, Tiempo, 5)` que contiene secuencias de vectores de landmarks faciales.\n",
    "\n",
    "**Proceso:**\n",
    "\n",
    "1. **Preprocesamiento de Im√°genes:**\n",
    "\n",
    "   * Se reordenan im√°genes en un √∫nico lote (`B*T`) para pasar todas por EfficientNet simult√°neamente.\n",
    "\n",
    "2. **Extracci√≥n de caracter√≠sticas visuales:**\n",
    "\n",
    "   * EfficientNet procesa las im√°genes para obtener embeddings visuales (`1280` dimensiones por imagen).\n",
    "\n",
    "3. **Combinar embeddings visuales y landmarks:**\n",
    "\n",
    "   * Las caracter√≠sticas visuales (`1280`) y los landmarks (`5`) se concatenan, formando un tensor combinado (`1285` dimensiones por frame).\n",
    "\n",
    "4. **Procesamiento secuencial con LSTM:**\n",
    "\n",
    "   * El tensor combinado pasa por la red LSTM bidireccional, que captura informaci√≥n temporal.\n",
    "\n",
    "5. **Clasificaci√≥n final:**\n",
    "\n",
    "   * Se utiliza solamente la √∫ltima salida de la secuencia LSTM (√∫ltimo frame procesado).\n",
    "   * Este vector final se introduce en la capa de clasificaci√≥n fully-connected.\n",
    "   * Se obtiene finalmente una predicci√≥n en forma de probabilidad.\n",
    "\n",
    "**Dimensiones de Tensores:**\n",
    "\n",
    "* Entrada: Im√°genes `(B, 16, 3, 256, 256)` y Landmarks `(B, 16, 5)`\n",
    "* CNN: `(B*16, 1280)`\n",
    "* Despu√©s CNN: `(B, 16, 1280)`\n",
    "* Combinado con landmarks: `(B, 16, 1285)`\n",
    "* Despu√©s LSTM: `(B, 16, 256)`\n",
    "* Salida final: `(B,)` (probabilidad binaria)\n",
    "\n",
    "---\n",
    "\n",
    "### **Resumen de uso:**\n",
    "\n",
    "El modelo es un detector especializado en analizar tanto patrones visuales como temporales en videos para identificar deepfakes, combinando t√©cnicas avanzadas de visi√≥n computacional y modelado secuencial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.cnn.classifier = nn.Identity()  # elimina la capa final\n",
    "        self.embedding_dim = 1280\n",
    "        self.sequence_length = 16\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=1285, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_imgs, x_lmks):\n",
    "        B, T, C, H, W = x_imgs.shape\n",
    "        x_imgs = x_imgs.view(B * T, C, H, W)\n",
    "        features = self.cnn(x_imgs)                     # (B*T, 1280)\n",
    "        features = features.view(B, T, -1)              # (B, T, 1280)\n",
    "        combined = torch.cat([features, x_lmks], dim=2) # (B, T, 1285)\n",
    "        out, _ = self.lstm(combined)\n",
    "        out = out[:, -1, :]                             # √∫ltima salida\n",
    "        return self.fc(out).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45924490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution:\n",
      "  Real (0): 768\n",
      "  Fake (1): 723\n",
      "------------------------------\n",
      "Validation class distribution:\n",
      "  Real (0): 207\n",
      "  Fake (1): 166\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:/Users/Hermanos/Desktop/Proyecto Deepfake/preprocesamiento_mediapipe\"\n",
    "\n",
    "train_dataset = DeepfakeDataset(base_path, split='train')\n",
    "val_dataset = DeepfakeDataset(base_path, split='val')\n",
    "\n",
    "print_class_distribution(train_dataset, \"Train\")\n",
    "print_class_distribution(val_dataset, \"Validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.6595 | Val Accuracy=0.7507 | Val F1-score=0.7424\n",
      "‚úÖ Modelo guardado con F1-score = 0.7424\n",
      "Epoch 2: Loss=0.5283 | Val Accuracy=0.8874 | Val F1-score=0.8750\n",
      "‚úÖ Modelo guardado con F1-score = 0.8750\n",
      "Epoch 3: Loss=0.3916 | Val Accuracy=0.8740 | Val F1-score=0.8698\n",
      "Epoch 4: Loss=0.3354 | Val Accuracy=0.9008 | Val F1-score=0.8969\n",
      "‚úÖ Modelo guardado con F1-score = 0.8969\n",
      "Epoch 5: Loss=0.2427 | Val Accuracy=0.9303 | Val F1-score=0.9217\n",
      "‚úÖ Modelo guardado con F1-score = 0.9217\n",
      "Epoch 6: Loss=0.1966 | Val Accuracy=0.7909 | Val F1-score=0.8079\n",
      "Epoch 7: Loss=0.1488 | Val Accuracy=0.9491 | Val F1-score=0.9408\n",
      "‚úÖ Modelo guardado con F1-score = 0.9408\n",
      "Epoch 8: Loss=0.1300 | Val Accuracy=0.9357 | Val F1-score=0.9245\n",
      "Epoch 9: Loss=0.0682 | Val Accuracy=0.9196 | Val F1-score=0.9143\n",
      "Epoch 10: Loss=0.0860 | Val Accuracy=0.9357 | Val F1-score=0.9264\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset y loaders\n",
    "train_dataset = DeepfakeDataset(base_path=r\"C:/Users/Hermanos/Desktop/Proyecto Deepfake/preprocesamiento_mediapipe\", split='train')\n",
    "val_dataset = DeepfakeDataset(base_path=r\"C:/Users/Hermanos/Desktop/Proyecto Deepfake/preprocesamiento_mediapipe\", split='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Modelo\n",
    "model = DeepfakeDetector().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, lmks, labels in train_loader:\n",
    "        imgs, lmks, labels = imgs.to(device), lmks.to(device), labels.to(device)\n",
    "        preds = model(imgs, lmks)\n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validaci√≥n\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lmks, labels in val_loader:\n",
    "            imgs, lmks = imgs.to(device), lmks.to(device)\n",
    "            outputs = model(imgs, lmks)\n",
    "            preds = (outputs > 0.5).cpu().numpy()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f} | Val Accuracy={acc:.4f} | Val F1-score={f1:.4f}\")\n",
    "\n",
    "    # Guardar mejor modelo\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save(model.state_dict(), \"mediapipe_model.pth\")\n",
    "        print(f\"‚úÖ Modelo guardado con F1-score = {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c9f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds: Counter({0: 213, 1: 160})\n"
     ]
    }
   ],
   "source": [
    "print(\"Preds:\", Counter(np.array(y_pred, dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdb5239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.93      0.96      0.94       207\n",
      "        fake       0.94      0.91      0.93       166\n",
      "\n",
      "    accuracy                           0.94       373\n",
      "   macro avg       0.94      0.93      0.93       373\n",
      "weighted avg       0.94      0.94      0.94       373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"real\", \"fake\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
