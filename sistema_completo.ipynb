{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7983feaf",
   "metadata": {},
   "source": [
    "# Sistema completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b107b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øGPU disponible?: True\n",
      "GPU actual: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"¬øGPU disponible?:\", torch.cuda.is_available())\n",
    "print(\"GPU actual:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Ninguna\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1bf6c",
   "metadata": {},
   "source": [
    "\n",
    "###  **Importaciones del Notebook y sus Instalaciones**\n",
    "\n",
    "Este notebook implementa diversas funcionalidades relacionadas con:\n",
    "\n",
    "* Detecci√≥n facial\n",
    "* Procesamiento de video e im√°genes\n",
    "* Reconocimiento √≥ptico de caracteres (OCR)\n",
    "* Automatizaci√≥n web\n",
    "* Verificaci√≥n de identidad facial y deepfake detection\n",
    "\n",
    "A continuaci√≥n se explica cada una de las importaciones y su uso:\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ **Importaciones utilizadas y explicaci√≥n:**\n",
    "\n",
    "##### Librer√≠as est√°ndar y generales:\n",
    "\n",
    "* **`os`**: Manejo de archivos y rutas.\n",
    "* **`time`**: Manejo de tiempos de espera y medici√≥n de tiempos.\n",
    "* **`re`**: Expresiones regulares, para extracci√≥n de CURP y fechas.\n",
    "\n",
    "##### Procesamiento de im√°genes y video:\n",
    "\n",
    "* **`cv2`** (`opencv-python`): Captura, procesamiento, visualizaci√≥n y guardado de im√°genes y video.\n",
    "* **`imutils`**: Herramienta que facilita tareas comunes con OpenCV (contornos, redimensionar im√°genes).\n",
    "\n",
    "##### Reconocimiento facial y detecci√≥n facial avanzada:\n",
    "\n",
    "* **`mediapipe`**: Para detecci√≥n facial y extracci√≥n de landmarks (puntos clave del rostro).\n",
    "* **`face_recognition`**: Comparaci√≥n facial para validar identidad.\n",
    "\n",
    "##### OCR (Reconocimiento √ìptico de Caracteres):\n",
    "\n",
    "* **`paddleocr`**: Reconocimiento de texto de la INE.\n",
    "\n",
    "##### Automatizaci√≥n web:\n",
    "\n",
    "* **`selenium`**: Automatizaci√≥n de consultas en p√°ginas web (sitio del CURP).\n",
    "\n",
    "##### Machine Learning (Deep Learning):\n",
    "\n",
    "* **`torch`** (`PyTorch`): Librer√≠a para deep learning.\n",
    "* **`torch.nn`**: Bloques fundamentales para construir redes neuronales.\n",
    "* **`torchvision`**: Modelos preentrenados (EfficientNet) y transformaci√≥n de im√°genes.\n",
    "* **`numpy`**: Manejo de arreglos num√©ricos y c√°lculos matriciales.\n",
    "\n",
    "\n",
    "##### **Instalaci√≥n completa de dependencias (`pip install`)**\n",
    "\n",
    "A continuaci√≥n, se presentan los comandos para instalar todas las librer√≠as necesarias. Es recomendable hacerlo en un entorno virtual:\n",
    "\n",
    "```bash\n",
    "# Procesamiento de im√°genes y video\n",
    "pip install opencv-python imutils\n",
    "\n",
    "# Detecci√≥n y reconocimiento facial\n",
    "pip install mediapipe face_recognition\n",
    "\n",
    "# OCR (PaddleOCR)\n",
    "pip install paddleocr paddlepaddle\n",
    "\n",
    "# Automatizaci√≥n web (Selenium)\n",
    "pip install selenium webdriver-manager\n",
    "\n",
    "# Machine Learning (PyTorch, con GPU o CPU)\n",
    "# Si tienes CUDA (GPU Nvidia):\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "\n",
    "# Si solo tienes CPU:\n",
    "pip install torch torchvision torchaudio\n",
    "\n",
    "# NumPy\n",
    "pip install numpy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Configuraci√≥n recomendada del entorno virtual:**\n",
    "\n",
    "Es altamente recomendable trabajar en un entorno virtual:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "\n",
    "# Activaci√≥n del entorno virtual:\n",
    "# Windows\n",
    ".venv\\Scripts\\activate\n",
    "\n",
    "# Linux/MacOS\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "Despu√©s, instala todas las dependencias listadas anteriormente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as est√°ndar\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Procesamiento de imagen y video\n",
    "import cv2\n",
    "import imutils\n",
    "\n",
    "# Automatizaci√≥n web (Selenium)\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# OCR con PaddleOCR\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# Detecci√≥n y reconocimiento facial\n",
    "import mediapipe as mp\n",
    "import face_recognition\n",
    "\n",
    "# Deep Learning (PyTorch y Torchvision)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9748e2",
   "metadata": {},
   "source": [
    "\n",
    "### Verificaci√≥n de Vida con Detecci√≥n Facial (MediaPipe)\n",
    "\n",
    "Esta secci√≥n del sistema implementa un reto de vida en tiempo real, que solicita al usuario realizar gestos espec√≠ficos (parpadeos, movimientos de cabeza) y verifica que la persona est√© frente a la c√°mara, centrada, con ojos abiertos y sin movimiento brusco, como paso previo a la validaci√≥n biom√©trica.\n",
    "\n",
    "---\n",
    "\n",
    "#### Inicializaci√≥n de MediaPipe y configuraci√≥n de √≠ndices\n",
    "\n",
    "```python\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "LEFT_EYE_FULL = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE_FULL = [362, 385, 387, 263, 373, 380]\n",
    "```\n",
    "\n",
    "* Se inicializa el m√≥dulo de malla facial de MediaPipe (`face_mesh`) y sus utilidades de dibujo (`drawing_utils`).\n",
    "* Se definen los √≠ndices de los puntos clave del ojo izquierdo y derecho para calcular el parpadeo mediante Eye Aspect Ratio (EAR).\n",
    "\n",
    "---\n",
    "\n",
    "#### Funci√≥n `eye_aspect_ratio()`\n",
    "\n",
    "```python\n",
    "def eye_aspect_ratio(landmarks, eye_indices, image_shape):\n",
    "    ...\n",
    "    return (A + B) / (2.0 * D)\n",
    "```\n",
    "\n",
    "* Calcula la relaci√≥n de aspecto del ojo (EAR), que disminuye cuando el ojo se cierra.\n",
    "* Se utiliza para detectar parpadeos reales.\n",
    "* Toma como entrada los landmarks faciales detectados, los √≠ndices del ojo y el tama√±o de la imagen.\n",
    "\n",
    "---\n",
    "\n",
    "#### Funci√≥n `is_looking_forward()`\n",
    "\n",
    "```python\n",
    "def is_looking_forward(landmarks):\n",
    "    ...\n",
    "    return left and right\n",
    "```\n",
    "\n",
    "* Verifica si el usuario est√° mirando al frente, evaluando la posici√≥n de la pupila relativa a los bordes de los ojos.\n",
    "* Esta informaci√≥n es clave para validar la toma de una selfie centrada con los ojos abiertos.\n",
    "\n",
    "---\n",
    "\n",
    "### Funci√≥n principal `realizar_reto_de_vida()`\n",
    "\n",
    "Esta es la funci√≥n central del sistema de verificaci√≥n facial en vivo.\n",
    "\n",
    "#### Objetivo:\n",
    "\n",
    "Verificar que el usuario:\n",
    "\n",
    "1. Est√° f√≠sicamente presente.\n",
    "2. Puede seguir instrucciones en tiempo real.\n",
    "3. No est√° usando una reproducci√≥n (deepfake o imagen fija).\n",
    "4. Est√° preparado para tomarse una selfie v√°lida.\n",
    "\n",
    "---\n",
    "\n",
    "#### Etapas del reto de vida\n",
    "\n",
    "1. **Captura de video en tiempo real**\n",
    "\n",
    "   * Se activa la c√°mara web (`cv2.VideoCapture(0)`) y se guarda el video completo como `verificacion_video.mp4`.\n",
    "\n",
    "2. **Detecci√≥n facial con MediaPipe**\n",
    "\n",
    "   * Se detectan landmarks faciales por frame.\n",
    "   * Se dibuja la malla facial sobre el rostro usando `mp_drawing.draw_landmarks()`.\n",
    "\n",
    "3. **Verificaci√≥n de parpadeos (‚â• 3)**\n",
    "\n",
    "   * Se calcula el EAR y se detectan cierres y aperturas del ojo.\n",
    "   * Se requiere que el usuario parpadee tres veces para superar esta etapa.\n",
    "\n",
    "4. **Detecci√≥n de movimientos de cabeza**\n",
    "\n",
    "   * Asentir (s√≠): movimiento vertical del rostro (eje Y).\n",
    "   * Negar (no): movimiento horizontal del rostro (eje X).\n",
    "   * Se requiere 2 gestos v√°lidos en cada caso.\n",
    "\n",
    "5. **Transici√≥n a la fase selfie**\n",
    "\n",
    "   * Al completar los gestos anteriores, el sistema cambia a la siguiente etapa.\n",
    "\n",
    "---\n",
    "\n",
    "#### Fase selfie autom√°tica\n",
    "\n",
    "Una vez completado el reto de vida, el sistema espera que el usuario est√©:\n",
    "\n",
    "* Centrado: nariz dentro de un rango horizontal y vertical.\n",
    "* Con ojos abiertos: EAR suficiente en ambos ojos.\n",
    "* Estable: pocos cambios en los p√≠xeles de los √∫ltimos frames.\n",
    "* Mirando al frente: pupilas centradas.\n",
    "\n",
    "Si todos los criterios se cumplen durante varios frames consecutivos, el sistema:\n",
    "\n",
    "* Captura la imagen y la guarda como `selfie.jpg`.\n",
    "* Finaliza exitosamente la funci√≥n con `return True`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Estructura del estado interno\n",
    "\n",
    "* `blink_counter`, `nod_count`, `shake_count`: contadores de gestos realizados.\n",
    "* `blink_completed`, `nod_completed`, `shake_completed`: estados de cada prueba.\n",
    "* `fase_selfie`: bandera que indica si se est√° en la fase de captura de selfie.\n",
    "* `gaze_start_time`, `stable_counter`: variables para validar centrado y estabilidad.\n",
    "\n",
    "---\n",
    "\n",
    "#### Resultado final\n",
    "\n",
    "* Si todos los pasos fueron completados y se captur√≥ la selfie, se devuelve `True`.\n",
    "* En caso contrario (interrupci√≥n o falla en los gestos), se devuelve `False`.\n",
    "\n",
    "---\n",
    "\n",
    "Este sistema permite detectar actividad humana aut√©ntica mediante gestos naturales y condiciones visuales, funcionando como una barrera efectiva contra suplantaciones por imagen o video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd35f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "LEFT_EYE_FULL = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE_FULL = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye_indices, image_shape):\n",
    "    def dist(p1, p2):\n",
    "        x1, y1 = int(p1.x * image_shape[1]), int(p1.y * image_shape[0])\n",
    "        x2, y2 = int(p2.x * image_shape[1]), int(p2.y * image_shape[0])\n",
    "        return ((x2 - x1)**2 + (y2 - y1)**2) ** 0.5\n",
    "    A = dist(landmarks[eye_indices[1]], landmarks[eye_indices[5]])\n",
    "    B = dist(landmarks[eye_indices[2]], landmarks[eye_indices[4]])\n",
    "    D = dist(landmarks[eye_indices[0]], landmarks[eye_indices[3]])\n",
    "    return (A + B) / (2.0 * D)\n",
    "\n",
    "def is_looking_forward(landmarks):\n",
    "    def centered(pupil, outer, inner):\n",
    "        d1 = abs(pupil - outer)\n",
    "        d2 = abs(inner - pupil)\n",
    "        ratio = d1 / (d1 + d2 + 1e-6)\n",
    "        return 0.425 < ratio < 0.575\n",
    "    left = centered(landmarks[468].x, landmarks[33].x, landmarks[133].x)\n",
    "    right = centered(landmarks[473].x, landmarks[362].x, landmarks[263].x)\n",
    "    return left and right\n",
    "\n",
    "def realizar_reto_de_vida():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_width, frame_height = int(cap.get(3)), int(cap.get(4))\n",
    "    out = cv2.VideoWriter(\"verificacion_video.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 20, (frame_width, frame_height))\n",
    "    face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "\n",
    "    # Estado\n",
    "    blink_counter, blink_ready = 0, True\n",
    "    blink_start_time, blink_completed = None, False\n",
    "    nod_count, nod_stage, nod_start_y, nod_start_time, nod_completed = 0, \"neutral\", None, None, False\n",
    "    shake_count, shake_stage, shake_start_x, shake_start_time, shake_completed = 0, \"neutral\", None, None, False\n",
    "    stable_frame, stable_counter = None, 0\n",
    "    gaze_start_time, selfie_taken = None, False\n",
    "    fase_selfie = False\n",
    "\n",
    "    print(\"üü° Iniciando reto de vida...\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        raw_frame = frame.copy()\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION)\n",
    "\n",
    "            ear_left = eye_aspect_ratio(face_landmarks.landmark, LEFT_EYE_FULL, frame.shape)\n",
    "            ear_right = eye_aspect_ratio(face_landmarks.landmark, RIGHT_EYE_FULL, frame.shape)\n",
    "            ear_avg = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            if not fase_selfie:\n",
    "                # Parpadeo\n",
    "                if not blink_completed:\n",
    "                    if ear_avg < 0.26 and blink_ready:\n",
    "                        if blink_counter == 0:\n",
    "                            blink_start_time = time.time()\n",
    "                        blink_counter += 1\n",
    "                        blink_ready = False\n",
    "                        print(f\"‚úÖ Parpadeo #{blink_counter}\")\n",
    "                    elif ear_avg >= 0.30:\n",
    "                        blink_ready = True\n",
    "                    if blink_counter > 0 and (time.time() - blink_start_time > 3):\n",
    "                        print(\"‚è±Ô∏è Parpadeos: tiempo excedido\")\n",
    "                        blink_counter, blink_start_time = 0, None\n",
    "                    if blink_counter >= 3:\n",
    "                        blink_completed = True\n",
    "                        print(\"‚úÖ Parpadeos completados\")\n",
    "\n",
    "                # Asentir (\"s√≠\")\n",
    "                nose_y = face_landmarks.landmark[1].y\n",
    "                if nod_start_y is None:\n",
    "                    nod_start_y = nose_y\n",
    "                    nod_start_time = time.time()\n",
    "                delta_y = nose_y - nod_start_y\n",
    "                if not nod_completed:\n",
    "                    if nod_stage == \"neutral\" and delta_y > 0.03:\n",
    "                        nod_stage = \"down\"\n",
    "                    elif nod_stage == \"down\" and delta_y < -0.03:\n",
    "                        nod_stage = \"up\"\n",
    "                        nod_count += 1\n",
    "                        print(f\"‚úÖ Asentimiento #{nod_count}\")\n",
    "                        nod_stage = \"neutral\"\n",
    "                    if time.time() - nod_start_time > 5 and nod_count < 2:\n",
    "                        print(\"‚è±Ô∏è Asentir: tiempo excedido\")\n",
    "                        nod_count, nod_start_time = 0, time.time()\n",
    "                    if nod_count >= 2:\n",
    "                        nod_completed = True\n",
    "                        print(\"‚úÖ Asentir completado\")\n",
    "\n",
    "                # Negar (\"no\")\n",
    "                nose_x = face_landmarks.landmark[1].x\n",
    "                if shake_start_x is None:\n",
    "                    shake_start_x = nose_x\n",
    "                    shake_start_time = time.time()\n",
    "                delta_x = nose_x - shake_start_x\n",
    "                if not shake_completed:\n",
    "                    if shake_stage == \"neutral\" and delta_x > 0.03:\n",
    "                        shake_stage = \"right\"\n",
    "                    elif shake_stage == \"right\" and delta_x < -0.03:\n",
    "                        shake_stage = \"left\"\n",
    "                        shake_count += 1\n",
    "                        print(f\"‚úÖ Negaci√≥n #{shake_count}\")\n",
    "                        shake_stage = \"neutral\"\n",
    "                    if time.time() - shake_start_time > 5 and shake_count < 2:\n",
    "                        print(\"‚è±Ô∏è Negar: tiempo excedido\")\n",
    "                        shake_count, shake_start_time = 0, time.time()\n",
    "                    if shake_count >= 2:\n",
    "                        shake_completed = True\n",
    "                        print(\"‚úÖ Negar completado\")\n",
    "\n",
    "                # Mostrar progreso\n",
    "                cv2.putText(frame, f\"Parpadeos: {blink_counter}/3\", (30, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                cv2.putText(frame, f\"Asentir: {nod_count}/2\", (30, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Negar: {shake_count}/2\", (30, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)\n",
    "\n",
    "                if blink_completed and nod_completed and shake_completed:\n",
    "                    print(\"üì∏ Reto de vida completado. Prepara tu selfie...\")\n",
    "                    fase_selfie = True\n",
    "                    start_selfie_time = time.time()\n",
    "                    continue\n",
    "\n",
    "            if fase_selfie and not selfie_taken:\n",
    "                nose_x = face_landmarks.landmark[1].x\n",
    "                nose_y = face_landmarks.landmark[1].y\n",
    "                centered = 0.4 < nose_x < 0.6 and 0.4 < nose_y < 0.6\n",
    "                eyes_open = ear_left > 0.26 and ear_right > 0.26\n",
    "\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
    "                if stable_frame is None:\n",
    "                    stable_frame = gray\n",
    "                    continue\n",
    "                diff = cv2.absdiff(stable_frame, gray)\n",
    "                _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
    "                movement = cv2.countNonZero(thresh)\n",
    "                stable = movement < 5000\n",
    "                stable_counter = stable_counter + 1 if stable else 0\n",
    "\n",
    "                looking_forward = is_looking_forward(face_landmarks.landmark)\n",
    "                if looking_forward:\n",
    "                    if gaze_start_time is None:\n",
    "                        gaze_start_time = time.time()\n",
    "                    gaze_duration = time.time() - gaze_start_time\n",
    "                else:\n",
    "                    gaze_start_time = None\n",
    "                    gaze_duration = 0\n",
    "\n",
    "                cv2.putText(frame, f\"Cara centrada: {'‚úÖ' if centered else '‚ùå'}\", (30, 140),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)\n",
    "                cv2.putText(frame, f\"Ojos abiertos: {'‚úÖ' if eyes_open else '‚ùå'}\", (30, 170),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)\n",
    "                cv2.putText(frame, f\"Estable: {'‚úÖ' if stable_counter >= 5 else '‚ùå'}\", (30, 200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)\n",
    "                cv2.putText(frame, f\"Mirando: {'‚úÖ' if gaze_duration >= 3 else f'{gaze_duration:.1f}s'}\", (30, 230),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)\n",
    "\n",
    "                if centered and eyes_open and stable_counter >= 5 and gaze_duration >= 3:\n",
    "                    cv2.imwrite(\"selfie.jpg\", raw_frame)\n",
    "                    print(\"üì∑ Selfie capturada como 'selfie.jpg'\")\n",
    "                    selfie_taken = True\n",
    "                    break\n",
    "                stable_frame = gray\n",
    "\n",
    "        cv2.imshow(\"Verificaci√≥n Facial\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"üé¨ Reto de vida completado correctamente.\")\n",
    "    if selfie_taken == True:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90601f",
   "metadata": {},
   "source": [
    "\n",
    "### Verificaci√≥n de INE contra el CURP oficial\n",
    "\n",
    "La funci√≥n `verificar_ine_con_curp()` se encarga de validar que una persona que muestra su INE frente a la c√°mara sea realmente quien dice ser. Este proceso combina visi√≥n por computadora, OCR y automatizaci√≥n web para verificar la correspondencia entre los datos visibles en la INE y los registrados en el sitio oficial del gobierno mexicano ([https://www.gob.mx/curp/](https://www.gob.mx/curp/)).\n",
    "\n",
    "---\n",
    "\n",
    "#### Funci√≥n interna `coincide_nombre_web_con_ocr()`\n",
    "\n",
    "Esta funci√≥n auxiliar compara si los datos extra√≠dos del OCR (nombre y apellidos) coinciden con los datos oficiales consultados en l√≠nea.\n",
    "\n",
    "```python\n",
    "def coincide_nombre_web_con_ocr(nombre_web, ap1, ap2, lineas_ocr):\n",
    "```\n",
    "\n",
    "* Recibe como entrada:\n",
    "\n",
    "  * Nombre y apellidos del sitio web oficial.\n",
    "  * L√≠neas de texto obtenidas con OCR desde la imagen de la INE.\n",
    "* Devuelve `True` si cada parte del nombre est√° presente en alguna l√≠nea OCR.\n",
    "* Se usa para validar la coincidencia textual entre el documento f√≠sico y la fuente oficial.\n",
    "\n",
    "---\n",
    "\n",
    "#### Inicializaci√≥n de componentes\n",
    "\n",
    "```python\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='es')\n",
    "```\n",
    "\n",
    "* Se inicializa el sistema OCR de PaddleOCR, configurado para idioma espa√±ol y detecci√≥n de orientaci√≥n del texto.\n",
    "\n",
    "```python\n",
    "chrome_options = Options()\n",
    "...\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "```\n",
    "\n",
    "* Se configuran opciones para ejecutar Chrome en modo sin interfaz gr√°fica (headless).\n",
    "* Se utiliza Selenium para automatizar la consulta al sitio del CURP.\n",
    "\n",
    "---\n",
    "\n",
    "#### Captura de la INE desde la c√°mara\n",
    "\n",
    "La funci√≥n inicia un bucle que:\n",
    "\n",
    "* Muestra instrucciones al usuario para que coloque su INE frente a la c√°mara.\n",
    "* Utiliza t√©cnicas de detecci√≥n de contornos con OpenCV para encontrar la tarjeta.\n",
    "* Verifica que la tarjeta se mantenga visible y estable durante una cantidad determinada de frames (`required_frames = 150`).\n",
    "* Una vez estable, guarda la imagen como `ine.jpg`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Extracci√≥n de datos con OCR\n",
    "\n",
    "Una vez que la INE es capturada:\n",
    "\n",
    "```python\n",
    "results = ocr.ocr(\"ine.jpg\", cls=True)\n",
    "```\n",
    "\n",
    "* Se procesa la imagen capturada con OCR.\n",
    "* Se extraen todas las l√≠neas de texto reconocidas y se convierten a may√∫sculas para comparaci√≥n.\n",
    "* Se buscan patrones espec√≠ficos:\n",
    "\n",
    "  * **CURP**: expresi√≥n regular para encontrar la cadena alfanum√©rica √∫nica.\n",
    "  * **Fecha de nacimiento**: en formato dd/mm/aaaa o dd-mm-aaaa.\n",
    "\n",
    "---\n",
    "\n",
    "#### Consulta al sitio oficial del CURP\n",
    "\n",
    "Usando Selenium:\n",
    "\n",
    "1. Se accede al sitio web oficial.\n",
    "2. Se introduce la CURP extra√≠da.\n",
    "3. Se recuperan los datos oficiales publicados (nombre, apellidos, fecha de nacimiento).\n",
    "4. Se compara:\n",
    "\n",
    "   * El nombre y apellidos con el OCR usando `coincide_nombre_web_con_ocr()`.\n",
    "   * La fecha con la extra√≠da del OCR.\n",
    "\n",
    "---\n",
    "\n",
    "#### Validaci√≥n final\n",
    "\n",
    "* Si tanto los nombres como la fecha coinciden, se considera que la persona fue verificada correctamente y se retorna `True`.\n",
    "* Si no coinciden, se indica que hubo una discrepancia y se retorna `False`.\n",
    "* Si ocurre un error en la conexi√≥n o carga de elementos del sitio, se captura la excepci√≥n, se limpia la sesi√≥n del navegador y se repite el proceso desde la captura de la INE.\n",
    "\n",
    "---\n",
    "\n",
    "#### Estructura de control\n",
    "\n",
    "La funci√≥n completa est√° envuelta en un `while True` para permitir reintentos autom√°ticos en caso de error (por ejemplo, si la INE no fue detectada correctamente o hubo un fallo en la consulta web).\n",
    "\n",
    "---\n",
    "\n",
    "### Resultados\n",
    "\n",
    "Esta funci√≥n automatiza la validaci√≥n de identidad documental utilizando una combinaci√≥n de:\n",
    "\n",
    "* Detecci√≥n visual de tarjetas de identificaci√≥n.\n",
    "* Reconocimiento √≥ptico de caracteres con PaddleOCR.\n",
    "* Automatizaci√≥n web con Selenium.\n",
    "* Verificaci√≥n cruzada con fuentes oficiales.\n",
    "\n",
    "Esto permite asegurar que el documento presentado pertenece realmente al usuario frente a la c√°mara y que no se trata de un intento de suplantaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_ine_con_curp():\n",
    "\n",
    "    def coincide_nombre_web_con_ocr(nombre_web, ap1, ap2, lineas_ocr):\n",
    "        partes = [nombre_web, ap1, ap2]\n",
    "        for parte in partes:\n",
    "            if not any(parte.upper() in linea for linea in lineas_ocr):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang='es')\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "    while True:\n",
    "        print(\"ü™™ Muestra tu INE al centro de la c√°mara, bien enfocada y estable...\")\n",
    "\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        stable_frame = None\n",
    "        ine_capturada = False\n",
    "        stable_counter = 0\n",
    "        required_frames = 150\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, raw_frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = raw_frame.copy()\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "            edged = cv2.Canny(blur, 75, 200)\n",
    "            cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cnts = imutils.grab_contours(cnts)\n",
    "            cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:5]\n",
    "\n",
    "            found_card = False\n",
    "            frame_area = frame.shape[0] * frame.shape[1]\n",
    "\n",
    "            for c in cnts:\n",
    "                approx = cv2.approxPolyDP(c, 0.02 * cv2.arcLength(c, True), True)\n",
    "                area = cv2.contourArea(c)\n",
    "                if len(approx) == 4 and area > 0.20 * frame_area:\n",
    "                    cv2.drawContours(frame, [approx], -1, (0, 255, 0), 2)\n",
    "                    stable_counter += 1\n",
    "                    found_card = True\n",
    "                    break\n",
    "\n",
    "            if not found_card:\n",
    "                stable_counter = 0\n",
    "\n",
    "            porcentaje = int((stable_counter / required_frames) * 100)\n",
    "            cv2.putText(frame, f\"INE detectada: {porcentaje}%\", (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "            cv2.putText(frame, \"Mant√©n la INE visible y grande durante 5 segundos\",\n",
    "                        (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            cv2.imshow(\"Detecci√≥n de INE\", frame)\n",
    "\n",
    "            if stable_counter >= required_frames:\n",
    "                cv2.imwrite(\"ine.jpg\", raw_frame)\n",
    "                print(\"üì∏ INE capturada como 'ine.jpg'\")\n",
    "                ine_capturada = True\n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == 27:\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                return False\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        if not ine_capturada:\n",
    "            continue\n",
    "\n",
    "        print(\"üîç Procesando INE con OCR...\")\n",
    "        results = ocr.ocr(\"ine.jpg\", cls=True)\n",
    "        lineas_ocr_ine = [line[1][0].strip().upper() for line in results[0]]\n",
    "\n",
    "        curp_match = next((re.search(r\"\\b[A-Z]{4}\\d{6}[A-Z0-9]{8}\\b\", l)\n",
    "                           for l in lineas_ocr_ine if re.search(r\"\\b[A-Z]{4}\\d{6}[A-Z0-9]{8}\\b\", l)), None)\n",
    "        curp = curp_match.group() if curp_match else \"No detectado\"\n",
    "\n",
    "        fecha_match = next((re.search(r\"\\b\\d{2}[/-]\\d{2}[/-]\\d{4}\\b\", l)\n",
    "                            for l in lineas_ocr_ine if re.search(r\"\\b\\d{2}[/-]\\d{2}[/-]\\d{4}\\b\", l)), None)\n",
    "        fecha = fecha_match.group() if fecha_match else \"No detectada\"\n",
    "\n",
    "        print(\"üìå CURP detectado:\", curp)\n",
    "        print(\"üìå Fecha de nacimiento:\", fecha)\n",
    "\n",
    "        print(\"üåê Consultando datos oficiales del CURP...\")\n",
    "        try:\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.get(\"https://www.gob.mx/curp/\")\n",
    "\n",
    "            input_curp = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"curpinput\"))\n",
    "            )\n",
    "            input_curp.send_keys(curp)\n",
    "            boton_buscar = driver.find_element(By.ID, \"searchButton\")\n",
    "            boton_buscar.click()\n",
    "\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//td[@style='text-transform: uppercase;']\"))\n",
    "            )\n",
    "\n",
    "            celdas = driver.find_elements(By.XPATH, \"//td[@style='text-transform: uppercase;']\")\n",
    "            if len(celdas) >= 7:\n",
    "                nombre_web = celdas[1].text.strip()\n",
    "                apellido1_web = celdas[2].text.strip()\n",
    "                apellido2_web = celdas[3].text.strip()\n",
    "                fecha_web = celdas[5].text.strip()\n",
    "\n",
    "                print(\"üìÑ Datos oficiales:\", apellido1_web, apellido2_web, nombre_web, fecha_web)\n",
    "                nombre_ok = coincide_nombre_web_con_ocr(nombre_web, apellido1_web, apellido2_web, lineas_ocr_ine)\n",
    "                fecha_ok = (fecha.strip() == fecha_web.strip())\n",
    "\n",
    "                if nombre_ok and fecha_ok:\n",
    "                    print(\"‚úÖ Identidad confirmada con datos oficiales.\")\n",
    "                    driver.quit()\n",
    "                    return True\n",
    "                else:\n",
    "                    print(\"‚ùå Los datos de la INE no coinciden con el CURP.\")\n",
    "            else:\n",
    "                print(\"‚ùå No se extrajeron suficientes datos del sitio.\")\n",
    "            driver.quit()\n",
    "            time.sleep(2)\n",
    "\n",
    "        except (WebDriverException, TimeoutException) as e:\n",
    "            print(f\"‚ö†Ô∏è Error en Selenium: {e}\")\n",
    "            print(\"üîÅ Ocurri√≥ un error al consultar el CURP. Volveremos a escanear la INE...\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(2)\n",
    "            continue  # vuelve al loop a escanear de nuevo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375e649",
   "metadata": {},
   "source": [
    "\n",
    "### Comparaci√≥n de rostro entre INE y selfie\n",
    "\n",
    "La funci√≥n `comparar_rostros_ine_selfie()` tiene como prop√≥sito verificar que el rostro detectado en la INE coincida con el rostro de la selfie capturada previamente durante el reto de vida. Esta verificaci√≥n biom√©trica fortalece el sistema de autenticaci√≥n asegurando que el documento pertenece a la persona que lo presenta.\n",
    "\n",
    "---\n",
    "\n",
    "#### Paso 1: Detecci√≥n del rostro en la INE\n",
    "\n",
    "```python\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.6)\n",
    "```\n",
    "\n",
    "* Se inicializa el detector facial de MediaPipe.\n",
    "* `model_selection=1` indica que se utilizar√° el modelo m√°s preciso para im√°genes cercanas.\n",
    "* `min_detection_confidence=0.6` establece el umbral m√≠nimo de confianza para considerar una detecci√≥n v√°lida.\n",
    "\n",
    "```python\n",
    "image = cv2.imread(\"ine.jpg\")\n",
    "```\n",
    "\n",
    "* Se carga la imagen previamente guardada de la INE.\n",
    "* Si no existe, se detiene el proceso y se retorna `False`.\n",
    "\n",
    "```python\n",
    "results = face_detection.process(image_rgb)\n",
    "```\n",
    "\n",
    "* La imagen es procesada para encontrar rostros.\n",
    "* Si se detecta al menos un rostro, se selecciona el de mayor √°rea (m√°s grande), suponiendo que es el rostro principal.\n",
    "\n",
    "```python\n",
    "cropped_face = image[y:y + h_box, x:x + w_box]\n",
    "cv2.imwrite(\"ine_face.jpg\", cropped_face)\n",
    "```\n",
    "\n",
    "* Se recorta y guarda el rostro detectado como `ine_face.jpg` para la posterior comparaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "#### Paso 2: Comparaci√≥n facial con la selfie\n",
    "\n",
    "```python\n",
    "if not os.path.exists(\"selfie.jpg\") or not os.path.exists(\"ine_face.jpg\"):\n",
    "```\n",
    "\n",
    "* Se valida la existencia de ambas im√°genes necesarias para la comparaci√≥n.\n",
    "\n",
    "```python\n",
    "img1 = face_recognition.load_image_file(\"selfie.jpg\")\n",
    "img2 = face_recognition.load_image_file(\"ine_face.jpg\")\n",
    "```\n",
    "\n",
    "* Ambas im√°genes son cargadas con la librer√≠a `face_recognition`.\n",
    "\n",
    "```python\n",
    "enc1 = face_recognition.face_encodings(img1)\n",
    "enc2 = face_recognition.face_encodings(img2)\n",
    "```\n",
    "\n",
    "* Se generan los vectores de caracter√≠sticas faciales (face embeddings) a partir de cada imagen.\n",
    "\n",
    "---\n",
    "\n",
    "#### Evaluaci√≥n de coincidencia\n",
    "\n",
    "```python\n",
    "result = face_recognition.compare_faces([enc1[0]], enc2[0])\n",
    "distance = face_recognition.face_distance([enc1[0]], enc2[0])[0]\n",
    "```\n",
    "\n",
    "* Se compara la similitud entre los vectores faciales mediante:\n",
    "\n",
    "  * `compare_faces()`: entrega `True` si los rostros coinciden.\n",
    "  * `face_distance()`: devuelve la distancia euclidiana entre embeddings; cuanto menor sea, m√°s parecidos son.\n",
    "\n",
    "**Resultado:**\n",
    "\n",
    "* Si los vectores coinciden (`result[0] == True`), se confirma la correspondencia facial y se retorna `True`.\n",
    "* Si no coinciden o si los vectores no pudieron generarse, se considera que no hay coincidencia y se retorna `False`.\n",
    "\n",
    "---\n",
    "\n",
    "Esta funci√≥n permite validar biom√©tricamente si la persona que aparece en la INE es la misma que realiz√≥ el reto de vida, fortaleciendo el sistema de verificaci√≥n de identidad contra intentos de suplantaci√≥n. Se basa en detecci√≥n de rostro (MediaPipe) y comparaci√≥n de caracter√≠sticas faciales (face\\_recognition) para lograr una evaluaci√≥n confiable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_rostros_ine_selfie():\n",
    "\n",
    "    print(\"üß† Detectando rostro en la INE...\")\n",
    "\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.6)\n",
    "\n",
    "    image = cv2.imread(\"ine.jpg\")\n",
    "    if image is None:\n",
    "        print(\"‚ùå No se encontr√≥ 'ine.jpg'.\")\n",
    "        return False\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(image_rgb)\n",
    "\n",
    "    if results.detections:\n",
    "        h, w, _ = image.shape\n",
    "        max_area = 0\n",
    "        best_face = None\n",
    "\n",
    "        for detection in results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            x, y = int(bbox.xmin * w), int(bbox.ymin * h)\n",
    "            w_box, h_box = int(bbox.width * w), int(bbox.height * h)\n",
    "            area = w_box * h_box\n",
    "\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "                best_face = (x, y, w_box, h_box)\n",
    "\n",
    "        if best_face:\n",
    "            x, y, w_box, h_box = best_face\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            cropped_face = image[y:y + h_box, x:x + w_box]\n",
    "            cv2.imwrite(\"ine_face.jpg\", cropped_face)\n",
    "            print(\"‚úÖ Rostro recortado y guardado como 'ine_face.jpg'\")\n",
    "    else:\n",
    "        print(\"‚ùå No se detect√≥ rostro en la INE.\")\n",
    "        return False\n",
    "\n",
    "    # Comparaci√≥n facial\n",
    "    print(\"üß™ Comparando rostro de INE con selfie...\")\n",
    "\n",
    "    if not os.path.exists(\"selfie.jpg\") or not os.path.exists(\"ine_face.jpg\"):\n",
    "        print(\"‚ùå Faltan im√°genes para la comparaci√≥n facial.\")\n",
    "        return False\n",
    "\n",
    "    img1 = face_recognition.load_image_file(\"selfie.jpg\")\n",
    "    img2 = face_recognition.load_image_file(\"ine_face.jpg\")\n",
    "\n",
    "    enc1 = face_recognition.face_encodings(img1)\n",
    "    enc2 = face_recognition.face_encodings(img2)\n",
    "\n",
    "    if enc1 and enc2:\n",
    "        result = face_recognition.compare_faces([enc1[0]], enc2[0])\n",
    "        distance = face_recognition.face_distance([enc1[0]], enc2[0])[0]\n",
    "\n",
    "        if result[0]:\n",
    "            print(f\"‚úÖ Rostros coinciden (distancia: {distance:.4f})\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Rostros NO coinciden (distancia: {distance:.4f})\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ùå No se pudieron codificar ambos rostros correctamente.\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdc581",
   "metadata": {},
   "source": [
    "\n",
    "### Predicci√≥n de Deepfake sobre un video capturado\n",
    "\n",
    "La funci√≥n `predecir_deepfake()` eval√∫a si un video contiene contenido manipulado (deepfake) utilizando un modelo previamente entrenado con secuencias de rostro y vectores de puntos faciales. Esta funci√≥n es la √∫ltima etapa de verificaci√≥n del sistema y act√∫a como filtro biom√©trico basado en redes neuronales.\n",
    "\n",
    "---\n",
    "\n",
    "#### Definici√≥n del modelo: `DeepfakeDetector`\n",
    "\n",
    "```python\n",
    "class DeepfakeDetector(nn.Module):\n",
    "```\n",
    "\n",
    "Este modelo combina:\n",
    "\n",
    "* **EfficientNet-B0** (preentrenado): como extractor de caracter√≠sticas visuales (embeddings de im√°genes faciales).\n",
    "* **LSTM bidireccional**: para capturar dependencias temporales entre los frames de un video.\n",
    "* **Red fully-connected**: que toma la √∫ltima salida del LSTM y produce una probabilidad de deepfake.\n",
    "\n",
    "**Entradas del modelo:**\n",
    "\n",
    "* Secuencia de im√°genes faciales (`x_imgs`)\n",
    "* Vectores de landmarks por frame (`x_lmks`)\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "* Valor entre 0 y 1 indicando la probabilidad de que el video sea un deepfake.\n",
    "\n",
    "---\n",
    "\n",
    "#### Preprocesamiento del video\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "```\n",
    "\n",
    "* El video de entrada es le√≠do con OpenCV.\n",
    "* Se seleccionan 25 frames distribuidos equitativamente a lo largo del video.\n",
    "* Se procesan hasta reunir 16 frames v√°lidos con rostro detectado.\n",
    "\n",
    "---\n",
    "\n",
    "#### Extracci√≥n de rostros y landmarks\n",
    "\n",
    "##### Funci√≥n `crop_face_from_landmarks()`\n",
    "\n",
    "* Recorta el rostro detectado en cada frame, utilizando los puntos de referencia proporcionados por MediaPipe.\n",
    "* Se a√±ade un margen alrededor del rostro para asegurar que est√© completo.\n",
    "* Las im√°genes se redimensionan a 256√ó256 p√≠xeles.\n",
    "\n",
    "##### Funci√≥n `extract_landmark_vector()`\n",
    "\n",
    "* Extrae un vector de 5 valores normalizados por frame:\n",
    "\n",
    "  * Coordenadas horizontales de los ojos.\n",
    "  * Coordenadas verticales de nariz y extremos de boca.\n",
    "* Este vector representa una firma geom√©trica b√°sica del rostro y se usa junto al embedding de imagen.\n",
    "\n",
    "---\n",
    "\n",
    "#### Almacenamiento de evidencia\n",
    "\n",
    "La primera imagen facial recortada del video se guarda como `\"evidencia.jpg\"` para auditor√≠a o respaldo visual del an√°lisis.\n",
    "\n",
    "---\n",
    "\n",
    "#### Verificaci√≥n de frames\n",
    "\n",
    "```python\n",
    "if len(images) < sequence_length:\n",
    "    return None\n",
    "```\n",
    "\n",
    "* Si no se re√∫nen al menos 16 frames v√°lidos, se cancela la predicci√≥n por falta de evidencia.\n",
    "\n",
    "---\n",
    "\n",
    "#### Carga del modelo y predicci√≥n\n",
    "\n",
    "```python\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "* Se carga el modelo entrenado desde un archivo `.pth`.\n",
    "* Se colocan las entradas en el dispositivo correspondiente (`cuda` o `cpu`).\n",
    "* Se realiza la inferencia con el modelo sin actualizar gradientes (`torch.no_grad()`).\n",
    "\n",
    "---\n",
    "\n",
    "#### Resultado final\n",
    "\n",
    "```python\n",
    "label = \"FAKE\" if prob > 0.5 else \"REAL\"\n",
    "```\n",
    "\n",
    "* Si la probabilidad es mayor a 0.5, el video se clasifica como deepfake.\n",
    "* Si es menor o igual, se considera real.\n",
    "* Se devuelve un diccionario con:\n",
    "\n",
    "  * Etiqueta (`label`)\n",
    "  * Probabilidad num√©rica (`prob`)\n",
    "  * Ruta de la imagen de evidencia (`evidencia.jpg`)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Esta funci√≥n permite realizar un an√°lisis autom√°tico sobre la autenticidad de un video capturado, combinando visi√≥n computacional, landmarks faciales y un modelo de detecci√≥n entrenado. Sirve como filtro final para confirmar que la persona que super√≥ el reto de vida y mostr√≥ su INE no est√° siendo suplantada mediante t√©cnicas de manipulaci√≥n audiovisual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_deepfake(video_path, model_path=\"mediapipe_model.pth\"):\n",
    "    \n",
    "    class DeepfakeDetector(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.cnn = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "            self.cnn.classifier = nn.Identity()\n",
    "            self.embedding_dim = 1280\n",
    "            self.sequence_length = 16\n",
    "\n",
    "            self.lstm = nn.LSTM(input_size=1285, hidden_size=128, num_layers=1,\n",
    "                                batch_first=True, bidirectional=True)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(256, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(64, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x_imgs, x_lmks):\n",
    "            B, T, C, H, W = x_imgs.shape\n",
    "            x_imgs = x_imgs.view(B * T, C, H, W)\n",
    "            features = self.cnn(x_imgs)\n",
    "            features = features.view(B, T, -1)\n",
    "            combined = torch.cat([features, x_lmks], dim=2)\n",
    "            out, _ = self.lstm(combined)\n",
    "            out = out[:, -1, :]\n",
    "            return self.fc(out).squeeze(1)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sequence_length = 16\n",
    "    candidate_frames = 25\n",
    "    image_size = (256, 256)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                                                   refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "    def extract_landmark_vector(landmarks, frame_shape):\n",
    "        h, w, _ = frame_shape\n",
    "        def norm(x): return x / w\n",
    "        def norm_y(y): return y / h\n",
    "        left_eye = landmarks.landmark[33]\n",
    "        right_eye = landmarks.landmark[263]\n",
    "        nose = landmarks.landmark[1]\n",
    "        mouth_left = landmarks.landmark[61]\n",
    "        mouth_right = landmarks.landmark[291]\n",
    "        return np.array([\n",
    "            norm(left_eye.x), norm(right_eye.x),\n",
    "            norm_y(nose.y),\n",
    "            norm_y(mouth_left.y),\n",
    "            norm_y(mouth_right.y)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def crop_face_from_landmarks(landmarks, frame):\n",
    "        h, w, _ = frame.shape\n",
    "        x_coords = [lm.x for lm in landmarks.landmark]\n",
    "        y_coords = [lm.y for lm in landmarks.landmark]\n",
    "        min_x, max_x = int(min(x_coords) * w), int(max(x_coords) * w)\n",
    "        min_y, max_y = int(min(y_coords) * h), int(max(y_coords) * h)\n",
    "        margin_x = int((max_x - min_x) * 0.2)\n",
    "        margin_y = int((max_y - min_y) * 0.2)\n",
    "        x1 = max(min_x - margin_x, 0)\n",
    "        y1 = max(min_y - margin_y, 0)\n",
    "        x2 = min(max_x + margin_x, w)\n",
    "        y2 = min(max_y + margin_y, h)\n",
    "        face_crop = frame[y1:y2, x1:x2]\n",
    "        return cv2.resize(face_crop, image_size)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, candidate_frames).astype(int)\n",
    "\n",
    "    images, landmarks_list = [], []\n",
    "    evidencia_guardada = False\n",
    "\n",
    "    for idx in frame_indices:\n",
    "        if len(images) >= sequence_length:\n",
    "            break\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = mp_face_mesh.process(rgb)\n",
    "        if results.multi_face_landmarks:\n",
    "            try:\n",
    "                lmks = results.multi_face_landmarks[0]\n",
    "                cropped = crop_face_from_landmarks(lmks, frame)\n",
    "                lmk_vector = extract_landmark_vector(lmks, frame.shape)\n",
    "\n",
    "                if not evidencia_guardada:\n",
    "                    cv2.imwrite(\"evidencia.jpg\", cropped)\n",
    "                    evidencia_guardada = True\n",
    "\n",
    "                images.append(transform(cropped))\n",
    "                landmarks_list.append(torch.tensor(lmk_vector, dtype=torch.float32))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    cap.release()\n",
    "    mp_face_mesh.close()\n",
    "\n",
    "    if len(images) < sequence_length:\n",
    "        print(f\"‚ö†Ô∏è Solo se obtuvieron {len(images)} frames v√°lidos. No se puede hacer inferencia.\")\n",
    "        return None\n",
    "\n",
    "    x_imgs = torch.stack(images[:sequence_length]).unsqueeze(0).to(device)\n",
    "    x_lmks = torch.stack(landmarks_list[:sequence_length]).unsqueeze(0).to(device)\n",
    "\n",
    "    model = DeepfakeDetector().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x_imgs, x_lmks)\n",
    "        prob = output.item()\n",
    "        label = \"FAKE\" if prob > 0.5 else \"REAL\"\n",
    "        print(f\"\\nüß™ Resultado: {label}  |  Probabilidad: {prob:.4f} | Evidencia: evidencia.jpg\")\n",
    "        return {\"label\": label, \"prob\": prob, \"evidencia\": \"evidencia.jpg\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467c9f8",
   "metadata": {},
   "source": [
    "Aqu√≠ tienes la explicaci√≥n en formato Markdown, sin emojis, que describe c√≥mo se utilizan las funciones previamente definidas en la ejecuci√≥n final del sistema:\n",
    "\n",
    "---\n",
    "\n",
    "## Flujo de ejecuci√≥n del sistema de verificaci√≥n de identidad\n",
    "\n",
    "El siguiente bloque de c√≥digo ejecuta de manera secuencial todo el proceso de validaci√≥n de identidad. Cada funci√≥n corresponde a una etapa cr√≠tica en el sistema y debe completarse correctamente para continuar con la siguiente.\n",
    "\n",
    "---\n",
    "\n",
    "### Orden y prop√≥sito de cada funci√≥n\n",
    "\n",
    "1. **`realizar_reto_de_vida()`**\n",
    "\n",
    "   * Verifica que el usuario est√© presente en tiempo real realizando gestos naturales (parpadeo, asentir y negar).\n",
    "   * Si se completa con √©xito, captura una selfie (`selfie.jpg`) y guarda un video (`verificacion_video.mp4`).\n",
    "\n",
    "2. **`verificar_ine_con_curp()`**\n",
    "\n",
    "   * Detecta y captura una imagen clara de la INE desde la c√°mara.\n",
    "   * Extrae datos mediante OCR y los compara con los datos oficiales consultados en el sitio del CURP.\n",
    "   * Solo si los nombres y la fecha de nacimiento coinciden, se considera v√°lida la identidad documental.\n",
    "\n",
    "3. **`comparar_rostros_ine_selfie()`**\n",
    "\n",
    "   * Detecta el rostro en la INE y lo compara con la selfie tomada anteriormente.\n",
    "   * Utiliza reconocimiento facial para confirmar que ambos rostros pertenecen a la misma persona.\n",
    "\n",
    "4. **`predecir_deepfake()`**\n",
    "\n",
    "   * Eval√∫a el video completo capturado durante el reto de vida para verificar si contiene se√±ales de manipulaci√≥n (deepfake).\n",
    "   * Devuelve una predicci√≥n final (`FAKE` o `REAL`) con la probabilidad asociada.\n",
    "\n",
    "---\n",
    "\n",
    "### Resultado final\n",
    "\n",
    "Si todas las etapas se completan exitosamente, se imprime el resultado final indicando si se trata de una persona real o no, basado en la predicci√≥n del modelo de detecci√≥n de deepfakes.\n",
    "\n",
    "En caso de que alguna de las etapas falle (ya sea por errores de lectura, detecci√≥n facial, discrepancia de datos o se√±ales de manipulaci√≥n), se cancela el flujo y se informa que no fue posible verificar la identidad de manera confiable.\n",
    "\n",
    "---\n",
    "\n",
    "Este flujo garantiza una validaci√≥n robusta, combinando detecci√≥n de actividad real, verificaci√≥n documental oficial, comparaci√≥n biom√©trica y an√°lisis de manipulaci√≥n digital.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ca8cae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü° Iniciando reto de vida...\n",
      "‚úÖ Parpadeo #1\n",
      "‚úÖ Parpadeo #2\n",
      "‚úÖ Negaci√≥n #1\n",
      "‚úÖ Parpadeo #3\n",
      "‚úÖ Parpadeos completados\n",
      "‚è±Ô∏è Asentir: tiempo excedido\n",
      "‚è±Ô∏è Negar: tiempo excedido\n",
      "‚úÖ Asentimiento #1\n",
      "‚úÖ Asentimiento #2\n",
      "‚úÖ Asentir completado\n",
      "‚úÖ Negaci√≥n #1\n",
      "‚è±Ô∏è Negar: tiempo excedido\n",
      "‚úÖ Negaci√≥n #1\n",
      "‚úÖ Negaci√≥n #2\n",
      "‚úÖ Negar completado\n",
      "üì∏ Reto de vida completado. Prepara tu selfie...\n",
      "üì∑ Selfie capturada como 'selfie.jpg'\n",
      "üé¨ Reto de vida completado correctamente.\n",
      "[2025/05/07 22:43:15] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, use_gcu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\rec\\\\latin\\\\latin_PP-OCRv3_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\Hermanos\\\\Desktop\\\\Proyecto Deepfake\\\\.venv-mediapipe\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\dict\\\\latin_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, onnx_providers=False, onnx_sess_options=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='es', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "ü™™ Muestra tu INE al centro de la c√°mara, bien enfocada y estable...\n",
      "üì∏ INE capturada como 'ine.jpg'\n",
      "üîç Procesando INE con OCR...\n",
      "[2025/05/07 22:44:15] ppocr DEBUG: dt_boxes num : 25, elapsed : 0.04928088188171387\n",
      "[2025/05/07 22:44:15] ppocr DEBUG: cls num  : 25, elapsed : 0.07489395141601562\n",
      "[2025/05/07 22:44:16] ppocr DEBUG: rec_res num  : 25, elapsed : 0.33364295959472656\n",
      "üìå CURP detectado: FOMJ030313HNLLNSA2\n",
      "üìå Fecha de nacimiento: 13/03/2003\n",
      "üåê Consultando datos oficiales del CURP...\n",
      "üìÑ Datos oficiales: FLORES MENDOZA JOSUE EMMANUEL 13/03/2003\n",
      "‚úÖ Identidad confirmada con datos oficiales.\n",
      "üß† Detectando rostro en la INE...\n",
      "‚úÖ Rostro recortado y guardado como 'ine_face.jpg'\n",
      "üß™ Comparando rostro de INE con selfie...\n",
      "‚úÖ Rostros coinciden (distancia: 0.4384)\n",
      "\n",
      "üß™ Resultado: REAL  |  Probabilidad: 0.3616 | Evidencia: evidencia.jpg\n",
      "Veredicto final: Es una persona REAL\n"
     ]
    }
   ],
   "source": [
    "if realizar_reto_de_vida() == True:\n",
    "    if verificar_ine_con_curp() == True:\n",
    "        if comparar_rostros_ine_selfie() == True:\n",
    "            resultado = predecir_deepfake(\"verificacion_video.mp4\", \"mediapipe_model.pth\")\n",
    "            if resultado:\n",
    "                print(\"Veredicto final: Es una persona\", resultado[\"label\"])\n",
    "            else:\n",
    "                print(\"‚ùå No se pudo analizar el video.\")\n",
    "        else:\n",
    "            print(\"No se pudo comprobar que se tratara de una persona real\")\n",
    "    else:\n",
    "        print(\"No se pudo comprobar que se tratara de una persona real\")\n",
    "else:\n",
    "    print(\"No se pudo comprobar que se tratara de una persona real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8532a",
   "metadata": {},
   "source": [
    "Resultado:\n",
    "\n",
    "```\n",
    "üü° Iniciando reto de vida...\n",
    "‚úÖ Parpadeo #1\n",
    "‚úÖ Parpadeo #2\n",
    "‚úÖ Negaci√≥n #1\n",
    "‚úÖ Parpadeo #3\n",
    "‚úÖ Parpadeos completados\n",
    "‚è±Ô∏è Asentir: tiempo excedido\n",
    "‚è±Ô∏è Negar: tiempo excedido\n",
    "‚úÖ Asentimiento #1\n",
    "‚úÖ Asentimiento #2\n",
    "‚úÖ Asentir completado\n",
    "‚úÖ Negaci√≥n #1\n",
    "‚è±Ô∏è Negar: tiempo excedido\n",
    "‚úÖ Negaci√≥n #1\n",
    "‚úÖ Negaci√≥n #2\n",
    "‚úÖ Negar completado\n",
    "üì∏ Reto de vida completado. Prepara tu selfie...\n",
    "üì∑ Selfie capturada como 'selfie.jpg'\n",
    "üé¨ Reto de vida completado correctamente.\n",
    "[2025/05/07 22:43:15] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, use_gcu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\rec\\\\latin\\\\latin_PP-OCRv3_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\Hermanos\\\\Desktop\\\\Proyecto Deepfake\\\\.venv-mediapipe\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\dict\\\\latin_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\Hermanos/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, onnx_providers=False, onnx_sess_options=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='es', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
    "ü™™ Muestra tu INE al centro de la c√°mara, bien enfocada y estable...\n",
    "üì∏ INE capturada como 'ine.jpg'\n",
    "üîç Procesando INE con OCR...\n",
    "[2025/05/07 22:44:15] ppocr DEBUG: dt_boxes num : 25, elapsed : 0.04928088188171387\n",
    "[2025/05/07 22:44:15] ppocr DEBUG: cls num  : 25, elapsed : 0.07489395141601562\n",
    "[2025/05/07 22:44:16] ppocr DEBUG: rec_res num  : 25, elapsed : 0.33364295959472656\n",
    "üìå CURP detectado: ********\n",
    "üìå Fecha de nacimiento: ********\n",
    "üåê Consultando datos oficiales del CURP...\n",
    "üìÑ Datos oficiales: FLORES MENDOZA JOSUE EMMANUEL ********\n",
    "‚úÖ Identidad confirmada con datos oficiales.\n",
    "üß† Detectando rostro en la INE...\n",
    "‚úÖ Rostro recortado y guardado como 'ine_face.jpg'\n",
    "üß™ Comparando rostro de INE con selfie...\n",
    "‚úÖ Rostros coinciden (distancia: 0.4384)\n",
    "\n",
    "üß™ Resultado: REAL  |  Probabilidad: 0.3616 | Evidencia: evidencia.jpg\n",
    "Veredicto final: Es una persona REAL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
