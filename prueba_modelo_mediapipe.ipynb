{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d304356",
   "metadata": {},
   "source": [
    "# Prueba del modelo con mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a265679",
   "metadata": {},
   "source": [
    "En este python notebook se encuentra el código para probar el modelo que verifica si es un deepfake o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c70e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "        self.cnn.classifier = nn.Identity()  # elimina la capa final\n",
    "        self.embedding_dim = 1280\n",
    "        self.sequence_length = 16\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=1285, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_imgs, x_lmks):\n",
    "        B, T, C, H, W = x_imgs.shape\n",
    "        x_imgs = x_imgs.view(B * T, C, H, W)\n",
    "        features = self.cnn(x_imgs)                     # (B*T, 1280)\n",
    "        features = features.view(B, T, -1)              # (B, T, 1280)\n",
    "        combined = torch.cat([features, x_lmks], dim=2) # (B, T, 1285)\n",
    "        out, _ = self.lstm(combined)\n",
    "        out = out[:, -1, :]                             # última salida\n",
    "        return self.fc(out).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d887bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import mediapipe as mp\n",
    "\n",
    "def predict_deepfake(video_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # === CONFIGURACIÓN ===\n",
    "    sequence_length = 16\n",
    "    candidate_frames = 25\n",
    "    image_size = (256, 256)\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                                                   refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "    def extract_landmark_vector(landmarks, frame_shape):\n",
    "        h, w, _ = frame_shape\n",
    "        def norm(x): return x / w\n",
    "        def norm_y(y): return y / h\n",
    "        left_eye = landmarks.landmark[33]\n",
    "        right_eye = landmarks.landmark[263]\n",
    "        nose = landmarks.landmark[1]\n",
    "        mouth_left = landmarks.landmark[61]\n",
    "        mouth_right = landmarks.landmark[291]\n",
    "        return np.array([\n",
    "            norm(left_eye.x), norm(right_eye.x),\n",
    "            norm_y(nose.y),\n",
    "            norm_y(mouth_left.y),\n",
    "            norm_y(mouth_right.y)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def crop_face_from_landmarks(landmarks, frame):\n",
    "        h, w, _ = frame.shape\n",
    "        x_coords = [lm.x for lm in landmarks.landmark]\n",
    "        y_coords = [lm.y for lm in landmarks.landmark]\n",
    "        min_x, max_x = int(min(x_coords) * w), int(max(x_coords) * w)\n",
    "        min_y, max_y = int(min(y_coords) * h), int(max(y_coords) * h)\n",
    "        margin_x = int((max_x - min_x) * 0.2)\n",
    "        margin_y = int((max_y - min_y) * 0.2)\n",
    "        x1 = max(min_x - margin_x, 0)\n",
    "        y1 = max(min_y - margin_y, 0)\n",
    "        x2 = min(max_x + margin_x, w)\n",
    "        y2 = min(max_y + margin_y, h)\n",
    "        face_crop = frame[y1:y2, x1:x2]\n",
    "        return cv2.resize(face_crop, image_size)\n",
    "\n",
    "    # === PROCESAR VIDEO ===\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, candidate_frames).astype(int)\n",
    "\n",
    "    images, landmarks_list = [], []\n",
    "    evidencia_guardada = False\n",
    "\n",
    "    for idx in frame_indices:\n",
    "        if len(images) >= sequence_length:\n",
    "            break\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = mp_face_mesh.process(rgb)\n",
    "        if results.multi_face_landmarks:\n",
    "            try:\n",
    "                lmks = results.multi_face_landmarks[0]\n",
    "                cropped = crop_face_from_landmarks(lmks, frame)\n",
    "                lmk_vector = extract_landmark_vector(lmks, frame.shape)\n",
    "\n",
    "                if not evidencia_guardada:\n",
    "                    cv2.imwrite(\"evidencia.jpg\", cropped)\n",
    "                    evidencia_guardada = True\n",
    "\n",
    "                images.append(transform(cropped))\n",
    "                landmarks_list.append(torch.tensor(lmk_vector, dtype=torch.float32))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    cap.release()\n",
    "    mp_face_mesh.close()\n",
    "\n",
    "    if len(images) < sequence_length:\n",
    "        print(f\"⚠️ Solo se obtuvieron {len(images)} frames válidos. No se puede hacer inferencia.\")\n",
    "        return\n",
    "\n",
    "    # === FORMATO TENSORES ===\n",
    "    x_imgs = torch.stack(images[:sequence_length]).unsqueeze(0).to(device)\n",
    "    x_lmks = torch.stack(landmarks_list[:sequence_length]).unsqueeze(0).to(device)\n",
    "\n",
    "    # === CARGAR MODELO Y PREDICCIÓN ===\n",
    "    model = DeepfakeDetector().to(device)\n",
    "    model.load_state_dict(torch.load(\"mediapipe_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x_imgs, x_lmks)\n",
    "        prob = output.item()\n",
    "        label = \"FAKE\" if prob > 0.5 else \"REAL\"\n",
    "        print(f\"\\nResultado: {label}  |  Probabilidad: {prob:.4f} | Evidencia guardada como evidencia.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e8802d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultado: REAL  |  Probabilidad: 0.4681 | Evidencia guardada como evidencia.jpg\n"
     ]
    }
   ],
   "source": [
    "predict_deepfake(\"C:/Users/Hermanos/Desktop/Proyecto Deepfake/verificacion_video.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
